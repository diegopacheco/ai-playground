## Paper

Tree of Thoughts: Deliberate Problem Solving wirh large language models
https://arxiv.org/abs/2305.10601

## Summary by Popai

In the research paper "Tree of Thoughts: Deliberate Problem Solving with Large Language Models," the authors address the limitations of existing language models (LMs) in problem-solving tasks and propose a new framework, "Tree of Thoughts" (ToT), to enhance LM inference. The authors highlight the confined nature of token-level, left-to-right decision-making processes in LMs, which may hinder their performance in tasks requiring exploration, strategic lookahead, or global decision-making. To overcome these challenges, the ToT framework allows LMs to perform deliberate decision-making by considering multiple reasoning paths and self-evaluating choices to decide the next course of action. By maintaining a tree of thoughts, each representing a coherent language sequence, ToT enables systematic exploration and backtracking to make more global decisions. The authors conduct experiments on three novel tasks—Game of 24, Creative Writing, and Mini Crosswords—to demonstrate the effectiveness of ToT in enhancing LM problem-solving abilities, achieving significant improvements in success rates compared to existing methods.

The introduction of ToT is inspired by human cognitive research, particularly the "dual process" models, which suggest that people engage in fast, automatic decisions (System 1) and slow, deliberate decisions (System 2). ToT aims to incorporate deliberate, conscious decision-making processes into LM inference, drawing on the planning processes explored by Newell, Shaw, and Simon in the 1950s. ToT allows for modularity and adaptability, as it can accommodate different problem properties, LM capabilities, and resource constraints. The authors break down the ToT framework into four key components: thought decomposition, thought generation, state evaluation, and search algorithms, providing a systematic approach to integrating deliberate decision-making into LM inference.

In the experiments, ToT demonstrates superior performance compared to IO prompting and CoT prompting in the Game of 24 task, achieving success rates of 45% and 74% with a breadth limit of 1 and 5, respectively, while IO and CoT methods achieve only 7.3% and 4.0% success rates. Additionally, in the Creative Writing task, ToT achieves higher coherence scores compared to IO and CoT methods, as well as outperforms CoT in human judgment evaluations. The authors also compare ToT with iterative-refinement methods, demonstrating the effectiveness of ToT in enhancing LM problem-solving capabilities.

Overall, the ToT framework presents a systematic and effective approach to enhancing LM problem-solving abilities by incorporating deliberate decision-making processes and systematic exploration. The experiments provide empirical evidence of the significant improvements in LM performance across various problem-solving tasks, highlighting the potential of ToT as a valuable framework for general problem-solving with LMs.
