# Arbitrage: Efficient Reasoning via Advantage-Aware Speculation

**arXiv ID**: 2512.05033
**PDF**: https://arxiv.org/pdf/2512.05033.pdf

---

### Overview
The paper titled "Arbitrage: Efficient Reasoning via Advantage-Aware Speculation" introduces ARBITRAGE, a novel framework aimed at optimizing the inference efficiency of large language models (LLMs) during reasoning tasks, particularly mathematical ones. The motivation arises from the substantial computational cost associated with auto-regressive decoding in LLMs. ARBITRAGE leverages a two-model setup, utilizing a fast draft model for initial token generation and a more capable target model for validation, with the aim of minimizing computational waste while maximizing output quality. Key datasets evaluated include multiple mathematical reasoning benchmarks such as MATH500 and OlympiadBench.

### Key Results
ARBITRAGE demonstrates impressive performance improvements over existing approaches, particularly Reward-guided Speculative Decoding (RSD). The key findings include:

- ARBITRAGE achieves up to **2× reduction in inference latency** while maintaining accuracy when compared to previous step-level speculative decoding baselines.
- When evaluated on specific datasets, ARBITRAGE consistently outperformed RSD at varying acceptance rates, indicating higher accuracy per unit of target usage across model configurations.
- On the MATH500 dataset with LLaMA3 configurations, the ARBITRAGE ROUTER shows a marked increase in accuracy, tracking closely with the upper bound set by the ARBITRAGE ORACLE.
- Particular setups demonstrated a striking **1.62× lower latency at a matched accuracy level** in the quantized-draft regime on MATH500, and **up to 1.97× speedup** on OlympiadBench when using a smaller draft model.

### Methodology
The methodology involved a dual model approach, where ARBITRAGE incorporates a lightweight router trained to predict the relative quality (advantage) of steps generated by the draft model compared to those from the target model. The training process for the router used a labeled dataset composed of 30,000 questions sampled from the NuminaMath-CoT dataset. Each data entry includes both draft and target model outputs along with their corresponding scores generated using a Process Reward Model (PRM). The router is trained to recognize when invocation of the target model would yield a better outcome than retaining the draft output. The evaluation metrics include accuracy and latency, with Spearman's rank correlation utilized as a proxy to measure the effectiveness of the router across different routing decisions.

### Critical Insights
Several critical insights emerged during the research:

- The authors highlight a significant inefficiency in traditional speculative decoding methods, particularly regarding its rejection process; many regenerations with the target model provided limited or no improvement in quality, leading to wasted computational efforts.
- An analysis indicates that as the deferral rate increases, the percentage of wasted computations (where the target model generates poorer or similar-quality outputs compared to the draft) also rises sharply.
- The qualitative assessments revealed that ARBITRAGE's advantage-aware routing could preserve coherent reasoning and minimize unnecessary regenerations, in contrast to RSD's rigid threshold mechanism which often discarded valid intermediate steps.
- The study notes that effective deployment of ARBITRAGE relies heavily on a carefully balanced training data set to improve routing performance, addressing potential biases inherent in step acceptance rates, where a significant portion of draft steps are generally valid yet still frequently rejected.

Overall, ARBITRAGE stands out in improving the efficiency of LLMs in reasoning tasks while tackling the inherent challenges associated with speculative decoding techniques.