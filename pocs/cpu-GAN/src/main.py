import numpy as np
from keras.layers import Input, Dense
from keras.models import Model
from keras.optimizers import Adam
from keras.models import Sequential
import matplotlib.pyplot as plt

# Define the generator model
def generator_model():
    model = Sequential()
    model.add(Dense(128, input_dim=100, activation='relu'))
    model.add(Dense(784, activation='sigmoid'))
    return model

# Define the discriminator model
def discriminator_model():
    model = Sequential()
    model.add(Dense(128, input_dim=784, activation='relu'))
    model.add(Dense(1, activation='sigmoid'))
    return model

# Define the combined generator and discriminator model
def combined_model(generator, discriminator):
    z = Input(shape=(100,))
    x_gen = generator(z)
    x_valid = discriminator(x_gen)
    model = Model(z, x_valid)
    return model

# Set the random seed
np.random.seed(0)

# Create the generator and discriminator models
generator = generator_model()
discriminator = discriminator_model()

# Compile the generator and discriminator models
generator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))
discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))

# Create the combined generator and discriminator model
combined = combined_model(generator, discriminator)

# Compile the combined model
combined.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))

# Number of examples to use for training
num_examples = 1000

# Generate real images (X_train)
# For a GAN that generates MNIST digits, real images would be 28x28 pixel images
X_train = np.random.rand(num_examples, 784)

# Generate labels for real images (y_train)
# Real images are labeled with 1
y_train = np.ones((num_examples, 1))

# Generate fake images (X_gen)
# Fake images are generated by the generator from random noise
noise = np.random.normal(0, 1, (num_examples, 100))
X_gen = generator.predict(noise)

# Generate labels for fake images (y_gen)
# Fake images are labeled with 0
y_gen = np.zeros((num_examples, 1))

# Generate random noise (z) for training the generator
z = np.random.normal(0, 1, (num_examples, 100))

# Train the GAN
for epoch in range(300):
    # Train the discriminator
    discriminator.trainable = True
    d_loss_real = discriminator.train_on_batch(X_train, y_train)
    d_loss_fake = discriminator.train_on_batch(X_gen, y_gen)
    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)
    
    # Train the generator
    discriminator.trainable = False
    g_loss = combined.train_on_batch(z, y_train)
    
    # Plot the progress
    print(f'Epoch {epoch+1}, Discriminator Loss: {d_loss}, Generator Loss: {g_loss}')

print('Training complete')

generator.save("gan.h5")
print('Model saved')

# prediction
noise = np.random.normal(0, 1, (1, 100))
generated_image = generator.predict(noise)

plt.imshow(generated_image.reshape(28, 28), cmap='gray')
plt.savefig('generated_image.png')
