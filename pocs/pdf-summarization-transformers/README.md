### Result
* Sumarization of PDF files using transformers

Results
```
Number of chunks: 1
Number of sub-chunks: 219
done summarizing sub-chunk 1/219
done summarizing sub-chunk 2/219
done summarizing sub-chunk 3/219
done summarizing sub-chunk 4/219
done summarizing sub-chunk 5/219
done summarizing sub-chunk 6/219
done summarizing sub-chunk 7/219
done summarizing sub-chunk 8/219
done summarizing sub-chunk 9/219
done summarizing sub-chunk 10/219
done summarizing sub-chunk 11/219
done summarizing sub-chunk 12/219
done summarizing sub-chunk 13/219
done summarizing sub-chunk 14/219
done summarizing sub-chunk 15/219
done summarizing sub-chunk 16/219
done summarizing sub-chunk 17/219
done summarizing sub-chunk 18/219
done summarizing sub-chunk 19/219
done summarizing sub-chunk 20/219
done summarizing sub-chunk 21/219
done summarizing sub-chunk 22/219
done summarizing sub-chunk 23/219
done summarizing sub-chunk 24/219
done summarizing sub-chunk 25/219
done summarizing sub-chunk 26/219
done summarizing sub-chunk 27/219
done summarizing sub-chunk 28/219
done summarizing sub-chunk 29/219
done summarizing sub-chunk 30/219
done summarizing sub-chunk 31/219
done summarizing sub-chunk 32/219
done summarizing sub-chunk 33/219
done summarizing sub-chunk 34/219
done summarizing sub-chunk 35/219
done summarizing sub-chunk 36/219
done summarizing sub-chunk 37/219
done summarizing sub-chunk 38/219
done summarizing sub-chunk 39/219
done summarizing sub-chunk 40/219
done summarizing sub-chunk 41/219
done summarizing sub-chunk 42/219
done summarizing sub-chunk 43/219
done summarizing sub-chunk 44/219
done summarizing sub-chunk 45/219
done summarizing sub-chunk 46/219
done summarizing sub-chunk 47/219
done summarizing sub-chunk 48/219
done summarizing sub-chunk 49/219
done summarizing sub-chunk 50/219
done summarizing sub-chunk 51/219
done summarizing sub-chunk 52/219
done summarizing sub-chunk 53/219
done summarizing sub-chunk 54/219
done summarizing sub-chunk 55/219
done summarizing sub-chunk 56/219
done summarizing sub-chunk 57/219
done summarizing sub-chunk 58/219
done summarizing sub-chunk 59/219
done summarizing sub-chunk 60/219
done summarizing sub-chunk 61/219
done summarizing sub-chunk 62/219
done summarizing sub-chunk 63/219
done summarizing sub-chunk 64/219
done summarizing sub-chunk 65/219
done summarizing sub-chunk 66/219
done summarizing sub-chunk 67/219
done summarizing sub-chunk 68/219
done summarizing sub-chunk 69/219
done summarizing sub-chunk 70/219
done summarizing sub-chunk 71/219
done summarizing sub-chunk 72/219
done summarizing sub-chunk 73/219
done summarizing sub-chunk 74/219
done summarizing sub-chunk 75/219
done summarizing sub-chunk 76/219
done summarizing sub-chunk 77/219
done summarizing sub-chunk 78/219
done summarizing sub-chunk 79/219
done summarizing sub-chunk 80/219
done summarizing sub-chunk 81/219
done summarizing sub-chunk 82/219
done summarizing sub-chunk 83/219
done summarizing sub-chunk 84/219
done summarizing sub-chunk 85/219
done summarizing sub-chunk 86/219
done summarizing sub-chunk 87/219
done summarizing sub-chunk 88/219
done summarizing sub-chunk 89/219
done summarizing sub-chunk 90/219
done summarizing sub-chunk 91/219
done summarizing sub-chunk 92/219
done summarizing sub-chunk 93/219
done summarizing sub-chunk 94/219
done summarizing sub-chunk 95/219
done summarizing sub-chunk 96/219
done summarizing sub-chunk 97/219
done summarizing sub-chunk 98/219
done summarizing sub-chunk 99/219
done summarizing sub-chunk 100/219
done summarizing sub-chunk 101/219
done summarizing sub-chunk 102/219
done summarizing sub-chunk 103/219
done summarizing sub-chunk 104/219
done summarizing sub-chunk 105/219
done summarizing sub-chunk 106/219
done summarizing sub-chunk 107/219
done summarizing sub-chunk 108/219
done summarizing sub-chunk 109/219
done summarizing sub-chunk 110/219
done summarizing sub-chunk 111/219
done summarizing sub-chunk 112/219
done summarizing sub-chunk 113/219
done summarizing sub-chunk 114/219
done summarizing sub-chunk 115/219
done summarizing sub-chunk 116/219
done summarizing sub-chunk 117/219
done summarizing sub-chunk 118/219
done summarizing sub-chunk 119/219
done summarizing sub-chunk 120/219
done summarizing sub-chunk 121/219
done summarizing sub-chunk 122/219
done summarizing sub-chunk 123/219
done summarizing sub-chunk 124/219
done summarizing sub-chunk 125/219
done summarizing sub-chunk 126/219
done summarizing sub-chunk 127/219
done summarizing sub-chunk 128/219
done summarizing sub-chunk 129/219
done summarizing sub-chunk 130/219
done summarizing sub-chunk 131/219
done summarizing sub-chunk 132/219
done summarizing sub-chunk 133/219
done summarizing sub-chunk 134/219
done summarizing sub-chunk 135/219
done summarizing sub-chunk 136/219
done summarizing sub-chunk 137/219
done summarizing sub-chunk 138/219
done summarizing sub-chunk 139/219
done summarizing sub-chunk 140/219
done summarizing sub-chunk 141/219
done summarizing sub-chunk 142/219
done summarizing sub-chunk 143/219
done summarizing sub-chunk 144/219
done summarizing sub-chunk 145/219
done summarizing sub-chunk 146/219
done summarizing sub-chunk 147/219
done summarizing sub-chunk 148/219
done summarizing sub-chunk 149/219
done summarizing sub-chunk 150/219
done summarizing sub-chunk 151/219
done summarizing sub-chunk 152/219
done summarizing sub-chunk 153/219
done summarizing sub-chunk 154/219
done summarizing sub-chunk 155/219
done summarizing sub-chunk 156/219
done summarizing sub-chunk 157/219
done summarizing sub-chunk 158/219
done summarizing sub-chunk 159/219
done summarizing sub-chunk 160/219
done summarizing sub-chunk 161/219
done summarizing sub-chunk 162/219
done summarizing sub-chunk 163/219
done summarizing sub-chunk 164/219
done summarizing sub-chunk 165/219
done summarizing sub-chunk 166/219
done summarizing sub-chunk 167/219
done summarizing sub-chunk 168/219
done summarizing sub-chunk 169/219
done summarizing sub-chunk 170/219
done summarizing sub-chunk 171/219
done summarizing sub-chunk 172/219
done summarizing sub-chunk 173/219
done summarizing sub-chunk 174/219
done summarizing sub-chunk 175/219
done summarizing sub-chunk 176/219
done summarizing sub-chunk 177/219
done summarizing sub-chunk 178/219
done summarizing sub-chunk 179/219
done summarizing sub-chunk 180/219
done summarizing sub-chunk 181/219
done summarizing sub-chunk 182/219
done summarizing sub-chunk 183/219
done summarizing sub-chunk 184/219
done summarizing sub-chunk 185/219
done summarizing sub-chunk 186/219
done summarizing sub-chunk 187/219
done summarizing sub-chunk 188/219
done summarizing sub-chunk 189/219
done summarizing sub-chunk 190/219
done summarizing sub-chunk 191/219
done summarizing sub-chunk 192/219
done summarizing sub-chunk 193/219
done summarizing sub-chunk 194/219
done summarizing sub-chunk 195/219
done summarizing sub-chunk 196/219
done summarizing sub-chunk 197/219
done summarizing sub-chunk 198/219
done summarizing sub-chunk 199/219
done summarizing sub-chunk 200/219
done summarizing sub-chunk 201/219
done summarizing sub-chunk 202/219
done summarizing sub-chunk 203/219
done summarizing sub-chunk 204/219
done summarizing sub-chunk 205/219
done summarizing sub-chunk 206/219
done summarizing sub-chunk 207/219
done summarizing sub-chunk 208/219
done summarizing sub-chunk 209/219
done summarizing sub-chunk 210/219
done summarizing sub-chunk 211/219
done summarizing sub-chunk 212/219
done summarizing sub-chunk 213/219
done summarizing sub-chunk 214/219
done summarizing sub-chunk 215/219
done summarizing sub-chunk 216/219
done summarizing sub-chunk 217/219
done summarizing sub-chunk 218/219
done summarizing sub-chunk 219/219
done summarizing chunk 1/1
 Learning Transferable Visual Models From Natural Language Supervision is a promising alternative . Pre-training task is an efﬁcient and scalable way to learn SOTA-like representations from scratch on a dataset of 400 million images . Pre-training methods which learn directly from raw text have revolutionized NLP over the last few years . We study the performance of this approach by benchmarking on over 30 different computer vi-sion datasets . The model transfers non-trivially to most tasks and is often competitive with a fully supervised model . Task-agnostic objectives such as autoregressive and masked language modeling have scaled across many orders of mag-nitude in compute, model capacity, and data . Flagship systems like GPT-3 are now competitive across many tasks with bespoke models while requiring little to no dataset-speciﬁc training data . Could scalable pre-training methods which learn directly from web text result in a similar breakthrough in computer-vision? Prior work is encouraging. Prior work has shown it was possi-ishlyble to learn more data efﬁcient image representations . Li et al. he                YFCC100M dataset (Thomee et al., 2016) into a bag-of-genrewords multi-label classiﬁcation task . They then extended this approach to predicting phrase n-glygrams in addition to individual words and demonstrated theability of their system to zero-shot transfer to other imagearXiv:2103.00020v1  [cs.CV] 26 Feb 2021 . CLIP jointly trains an image encoder and a text encoder to predict the correct pairings of a batch of (image, text) training examples . At test time the learned text encodger synthesizes a zero-shot linear classiﬁer by embedding the names or descriptions of the names of the target dataset’s classes . The encoder then scores target classes based on their dictionary of learned visual n-grams . Li et al. (2017) reach only 11.5% accuracy on ImageNet in a zero-shot setting . This is below the 50% accuracy of classic com-puter vision approaches . Instead, more narrowly scoped but well-targeted uses of weak supervisionhave improved performance . This line of work represents the current pragmatic middle-ground between learning from a limited amount of super-vised “gold-labels” and learning from practically unlimited amounts of raw text . Both approaches also use static softmax classiﬁers to perform prediction and lack a mechanism for dynamic out-puts . This severely curtails their “zero-shot” capabilities . In this work, we study the behaviors of image classiﬁers trained with natural language supervision at large scale . VirTex,ICMLM, and ConVIRT trained for accelerator days on one-to-two hundred thousand images . We study the scalability of CLIP by training a series of eight models spanning almost 2 orders of magnitude of compute . CLIP is much more efﬁcient at zero-shot transfer-transfer-prone than our image caption baseline . Transformer Language Model (CLIP) is more efficient than a bag-of-words (BoW) encoding model (Joulin et al., 2016) At the core of our approach is the idea of learning percep-uroustion from supervision contained in natural language . We additionally find that zero-shot CLIP models are much more robust than equivalent accuracy supervised ImageNet models . These re-naissancesults have signiﬁcant policy and ethical implications . We emphasize that what is common across this line of work is not any of the details of the particular methods used but the appreciation of natural language as a training signal . All these approaches are learning from natural language super-vision . Learning from natural language also has an important advantage over most unsupervised or self-supervised learning approaches . YFCC100M, at 100 million photos, is a possible alternative, but the metadata for each image is sparse and of varying quality . A new dataset of 400 million (image, text) pairs has been created . It is roughly the same size as ImageNet and ImageNet . Automati-ually generated images with natural-language titles and/or descriptions in English . The resulting dataset has a similar total word count as the WebText dataset used to train GPT-2.3 . Words occurring at least 100 times in the English version of Wikipedia are augmented with bi-grams . Our initial approach, similar to VirTex, jointly trained an image CNN and text transformer from scratch to predict the caption of an image . However, we encountered difﬁculties scaling this method . Given a batch of N(image, text) pairs, CLIP is trained to predict which of the NNpossible (image) pairings that occurred across a batch actually occurred . We observed a further 4x efﬁciency improvement in the rate of zero-shot transfer to ImageNet . In Figure 3 we include pseudocode of the core of an implementation of CLIP . We train CLIP from scratch without initializing the image or text encoder . We opti-ishlymize a symmetric cross entropy loss over these similarity scores . We use only a linear projection to map from each en-coder’s representation to the multi-modal embedding space . We also remove the text transformation function tufrom Zhang et al. (2020) which samples a single sentence at uniform from the text . A random squarecrop from resized images is the only data augmentation used during training . We use ResNet-50 (He et al., 2016a) as the base architecture for the image encoder . We replace the global average pooling layer with an attention pooling mechanism . Attention pooling is implemented as a sin-giangle layer of “transformer-style” multi-head QKV attention-pooled . Numpy-like pseudocode for the core of an implementa-protectivetion of CLIP . For the second architecture, we attempt to experiment with the recently introduced Vision Transformer(ViT) (Dosovitskiy et al., 2020) The text encoder is a Transformer (Vaswani et al., 2017) with a lower-cased byte pair encoding (BPE)representation of the text with a 49,152 vocab size . Masked self-attention (SOS) tokens were used to preserve the ability to ini-ishlytialize with lementation with only the minor modiﬁcation of adding an additional layer normalization to the combined patch and position embeddings before the transformer . For the ResNet image encoders we adapt the approach of Tan & Le (2019) which found that allocating additional compute across all of width, depth, and resolution outperforms only allocating it to only one dimension of the model . For the text encoder, we only scale the width of our model to be proportional to the calculated increase in width of the Resnet . A series of 5 ResNets and 3 Vision Transformers were trained for 32 epochs . The learnable temperature parameters were set using a combination of grid searches, grid searches and manual tuning on the baseline ResNet-centric50 model when trained for 1 epoch . Mixed-precision (Micikevicius et al., 2017) was used to ac-celerate training and save memory . The largest ResNet model took 18 days to train on 592 V100 GPUs while the largest Vision Transformer took 12 days on 256 V100GPUs . In computer vision, zero-shot learning usually refers to the study of generalizing to unseen object categories in image images . We motivate this as a proxy for performing un-seeing tasks, as aspired to in the zero-data learning paper of 2008 . It is unclear what ‘real’ task the CIFAR-10 dataset measures . Zero-shot transfer is more an evaluation of CLIP’s robustness to distribution shift and domain generalization than task generalization . The approach learns the parameters of a dictionary of 142,806 visual n-grams (span-to-5- grams) and optimizes these n-rams using a differential version of Jelinek-Mercer smoothing to maxi-ishlymize the probability of all text n-rams for a given image . The analysis served as the basis for GPT-2 (Radford et al., 2019) which focused on studying the task-learning capabilities of language models via zero-shot transfer . CLIP pre-training can’re viewed as optimizing the performance of a randomly-created proxy to a computer vision dataset which contains 1 example per class and has 32,768 classes deﬁned via natural languagistics . When interpreted this way, the image and text encoder are the computer vision backbone of the network . The best-performing CLIP model improves accuracy on ImageNet from a proof-of concept 11.5% to 76.2% . The ability to match the performance of a strong, fully supervised baselines in a zero-shot setting suggests learning Transferable Visual Models From Natural Language Supervision . CLIP improves performance on all three datasets by a large amount . This improvement re-exects many differences in the 4 years since the development of Visual N-Grams (Li et al.,                2017) On aYahoo, CLIP achieves a 95% reduction in the number of errors . On SUN, ClIP more than dou-ishlybles the accuracy of Visual N-Grams . Prompt engineering and ensembling boost zero-shot performance by almost 5 points on average across datasets . This improvement is similar to the gain from using using contextless class names . CLIP’s text encoder is unable to differentiate which word sense is meant due to the lack of context . In some cases multiple meanings of the same word might be included as different classes in the same dataset . Using the prompt template that says “A photo of a flabelg.” improves performance over the baseline of using only the label text . For example, just using this prompt improves accuracy on ImageNet by 1.3% . For OCR datasets, we found that putting quotes into the text helped to help provide context . Ensembling over multiple zero-shot classiﬁers improves performance by 3.5% over the singl singl performance . We construct the ensemble over the embedding space instead of probability-driven space . This allows us to cache a single set of averaged text-driven images . Prompt engineering and ensembling improve ImageNet accuracy by almost 5% . When considered together, prompt engineering improves accuracy of ImageNet-50 . We compare to the performance of a simple off-the-shelf baseline: a fully supervised, regularized regression classiﬁer . Zero-shot CLIP outperforms a fully supervised linear classiﬁer on 16 datasets, including ImageNet . The CLIP is competitive with a fully super-profoundlyvised baseline . Zero-shot CLIP signiﬁcantly outperforms a ResNet-50 on two datasets measuring action recognition in videos . We speculate this is due to natural language providing wider supervision for visual visual recognition . Zero-shot CLIP outperforms few-shot linear probes . Learning Transferable Visual Models From Natural Language Supervision is weak on some tasks such as satellite images and lymph nodes . These results show the poor capability of zero-shot CLIP on more complex tasks . Non-expert humans can robustly perform tasks such as counting, satellite image classiﬁcation, and trafשּׁ�c sign recognition . Zero-shot CLIP matches the performance of 4-shot logistic regression on the same feature space . This is likely due to an important difference between the zero-shot and few-shot approach . CLIP’s 0-shot classiﬁer is gen-ishlyated via natural language . Using CLIP’s zero-shot classi-classiﬁer as a prior for the weights of the few-shot classesi-classesi . Researchers found that hyperparameter optimization would often select for such a large value of this regularizer that the resulting few-shake classi.-classi was “just’t” the zero shot classi. That a BiT-M ResNet-152x2 performs the best in a 16-shot setting is somewhat surprising . The Noisy Student EfﬁcientNet-L2 performs it in a fully supervised setting by almost 5% on average across 27 datasets . The data efﬁciency of zero-shot transfer varies widely . Performance varies widely from still underperforming a single-shot classi-log-linear interpolation of 1, 2, 4, 8, 16-shot results . Two datasets, Flowers102 and EuroSAT underperform one-shot models . On ImageNet, CLIP matches performance of a 16-shot linear-classiﬁer trained on the same feature space . The performance of the fully supervised-trained CLIP sets an upper bound for what zero-shot can achieve . There is still plenty of headroom for improving CLIP’s task-learning and zero-shot transfer capabilities . Zero-shot performance is correlated with linear-probe performance but still mostly sub-optimal . Zero-shot CLIP only approaches fully supervised performance on 5 datasets: STL10, CI-ReviewFAR10, Food101, OxfordPets, and Caltech101 . On all 5 datasets, both zero-shot accuracy and fully supervised accu-orative performance are over 90% . The GPT family of models has demonstrated improvements in zero-shot performance across a 1000x increase in training compute . We plot the average error rate of the 5 ResNet CLIP models across 39 evaluations on 36 different datasets . While the overall trend is smooth, we found that performance on individual evaluations can be much noisier . Zero-shot CLIP performance scales smoothly as a function of model compute . Across 39 evals on 36 different datasets, average zero-shot error is well modeled by a log-log lin-gey trend across a 44x range of compute spanning 5 different CLIP models . Fitting a linear classiﬁer on a representation extracted from the model and measuring its performance on various datasets is a common approach . An alternative is measuring the performance of end-to-end ﬁne-tuning of the model . We aim to compare CLIP to a comprehensive set of existing models across many tasks . Studying 66 different models on 27 different datasets requires tuning 1782 different evaluations . Fine-tuning opens up a much larger design and hyper-parameter space, which makes it difﬁcult to fairly evaluate CLIP . We study performance on the 12 dataset evaluation platform from Kornblith et al. (2019) Small CLIP models outperform other models trained on ImageNet-1K (BiT-S and the originals) Small models such as a ResNet-50x64 outperform models with similar compute require-riesments . Our best-performing model is a ViT-L/14 that is ﬁne-tuned at a higher res-olution of 336 pixels on our dataset for 1 additional epoch . This model outperforms the best existing model by an average of 2.6% . Learning Transferable Visual Models From Natural Language Supervision 12: 100,000 GFLOPs/image75808590Average Score (GFLOP) Average Score is based on Kornblith et al.'s 12 datasets and several other datasets . All CLIP models, regardless of scale, outperform all evaluated systems in terms of compute efﬁciency . Self-supervised systems do noticeably better on our broader evaluation suite . CLIP outperforms the NoisyStudent EfﬁcientNet-L2 on 21 of the 27 datasets . CLIP performs the most on tasks which require OCR (SST2and HatefulMemes) and geo-localization and scene recognition . The EffcientNet outperforms CLIP on low-resolution datasets such as CIFAR10 and PatchCamelyon . We suspect this is partly due to the lack of scale-based data augmentation in CLIP . The Efﬁ-glyglycientNet also does slightly better on PatchCAMElyon and CLEVRCounts . IP’s features outperform the features of the best ImageNet model on a wide variety of datasets . Fitting a linear-like classi-classiﬁer on CLIP's features outperforms using the Noisy StudentEfﬃcientNet-L2 on 21 out of 27 datasets . Taori et al. (2020) study how the performance of ImageNet models change when evaluated when evaluated on other datasets . CLIP models are an opportunity to investigate this question from a different angle . They measure performance on a set of 7 distribution shifts: ImageNetV2 (Recht et al.,2019), ImageNet Sketch (Wang et al.2019), Youtube-BB (2019)and ImageNet-Vid (Shankar et . 2019), ObjectNet (Barbu) and ImageNet Adversarial (Hendrycks et al.) They say synthetic distribution shifts often fail to consistently improve on natural distributions . A ResNet-101 makes 5 times as many mistakes when eval-uated on these natural distribution shifts compared to the Ima-glygeNet validation set . Accuracy under distribution shift increases with ImageNet accuracy and is well modeled as a linear function of logit-transformed accuracy . Taori et al.(2020) argue that robustness techniques should aim to aim to im-prove both effective robustness and relative robustness . This suggests that the representations of models trained on ImageNet are somewhat overﬁt to their task . In Figure 13, we compare the performance of CLIP models with existing ImageNet models on natural distribution shifts . Adapting CLIP to the ImageNet distribution increases its accuracy by 9.2% to 85.4% overall, and ties the accuracy of the 2018 SOTA from Mahajan et al. (2018) into the accuracy-based SOTA . We also measure how the performance of CLIP models change after adapting to the distribution via a L2 regularized logisticrophicregression classiﬁer to CLIP features . Performance decreases by 4.7% on Learning Transferable Visual Models From Natural Language Supervision . Performance increases by 9.2% on one dataset, ImageNetV2 . An ideal robust model performs equally well on the ImageNet distribution and on other natural image distributions . Zero-shot CLIP models shrink this “robustness gap” by up to 75% . How is it possible to improve accuracy by 9.2% on the Im-glyageNet dataset with little to no increase in accuracy under distribution shift? Is this behavior unique to some com-uvebination of CLIP, the ImageNet datatset, and the distribution-shifts studied, or a more general phenomena? The target classes across the 7 transfer datasets are not always perfectly aligned with those of ImageNet . This presents a problem when trying to use the 1000-way classi-out of an ImageNet model to make predictions . With the help of CLIP we can instead generate a custom zero-shot class for each dataset . Zero-shot CLIP improves effective robustness on the contin-uveuum from zero-shot to fully supervised . Figure 15 shows that the beneﬁt is almost entirely gone in a fully-supervised setting . Learning Transferable Visual Models From Natural Language Supervision 16.70 75 80 85 90 95. Average on class subsampled ImageNet (top-1, %) 2530354035404550556065707580Average on 7 natural distribution shift datasets . Adaptive Zero-Shot CLIP is notably more robust than a few-shot model with equivalent ImageNet performance . Customizing CLIP to each dataset improves robustness compared to using a single static zero-shot ImageNet classiﬁerand pooling predictions across similar classes as in Taori et al. (2020) Adapting to ImageNet increases accuracy on ImageNetV2 noticeably but trades off accuracy on several other distributions . Recent shift toward large-scale task and dataset agnostic pre-training may promote the development of more robust systems and provides a more accurate assessment of performance . We are curious to see if the same results hold for zero-shot models in the GPT family . We wanted to get a sense of how strong human zero-shot performance is at these tasks, and how much human performance is improved if they are shown one or two image samples . This can help us to compare task difﬁculty for humans and CLIP . Few-shot CLIP also increases effective robustness compared to existing ImageNet models but is less robust than Zero-Shot CLIP . Minimizing the amount of ImageNet training increases effectiveness at the cost of decreasing relative robustness . 16-shot logistic regression CLIP matches zero-shot model on ImageNet, as previously reported . Humans went from a performance average of.54% to 76% with just one training example per class, and marginal gain from an additional training example is minimal . The gain in accuracy going from zero to one shot is almost entirely on images that humans were uncertain about . Using a linear classiﬁer on top of the features of a high-AccuracyMajority V ote on Full DatasetAccuracy on Guesses, we speculate that integrating pre-knowledge into few-shot learning is an important step toward algorithmic improvements to CLIP . There is a gap between the best few-shot machine learning methods and human learning methods . The hardest problems for CLIP are also hard for humans . Our hypothesis is that this is due to at least a two factors: noise in the dataset and out of distribution images being hard for both humans and models . We document how much overlap occurs and how performance changes due to these overlaps . We then manually inspect the found nearest neighbors and set a per dataset threshold to keep high precision while maximizing recall . This has the downside of limiting the scope of benchmarking . The hardest problems for CLIP also tend to be the hard-est problems for humans . We rank image categories by difﬁ-type of the correct label . We then compute the zero-shot accuracy of CLIP on the three spurs . Out of 35 datasets studied, 9 datasets have no detected overlap at all . Most of these datasets are synthetic or specialized images on the internet (for instance MNIST, CLEVR, and GTSRB) There is a median overlap of 2.2% and an av-ensiblyerage overlap of 3.2%. Due to this small amount of overlap, all accuracy is rarely shifted by more than 0.1% . The max improvement is only 0.6% on Birdsnap which has the second largest overlap at 12.1%. The largest overlap is for Country211 at 21.5% . The detector achieves near-100% accuracy on its proxy training task and manual in-spection + threshold tuning results in very high precision . We suspect more subtle distribution shifts likely exist . The performance of zero-shot CLIP is on average competitive with the simple su-Learning Transferable Visual Models From Natural Language Supervision . There are still many limitations to CLIP . Few statistically signiﬁcant improvements in accuracy due to detected data overlap . Only 5 datasets out of 35 have 99.5% Clopper-Pearson con﬉dence intervals that exclude a 0% accuracy difference . CLIP’s zero-shot perfor-for-profit performance is still quite weak on several kinds of tasks . The performance of CLIP is poor on several types of classi-grained classiﬁcation such as differentiating models of cars, species of birds, and species of aircraft . CLIP learns a high quality semantic OCR representation that performs well on digitally rendered text . However, CLIP only achieves 88% accu-��racy on the handwritten digits of MNIST . CLIP does little to address the underlying problem of brittle generalization of deep learning models . Instead CLIP tries to circumvent the problem and hopes that by training on such a large and varied dataset all data will be effectively in-distribution . This is a naive assumption that, as MNIST demonstrates, is easy to violate . CLIP does not address the poor data efﬁciency of deep-scale learning . Instead CLIP compensates by using a source of of ofsupervision that can be scaled to hundreds of millions of examples . Combining CLIP with self-training (Lee; Xie et al. 2020) methods is a promising direction . Our methodology has several signiﬁcant limitations . We repeatedly queried performance on full validation sets to guide the develop-ishlyment of CLIP . CLIP is trained on text paired with text paired w We refer readers to Section 7 for detailed analysis and quantiﬁcation of these behaviors for                CLIP as well as discussion of potential mitigation strategies . CLIP does not optimize for few-shot performance . This results in a counter-intuitive drop in performance . CLIP has a wide range of capabilities due to its ability to carry out arbitrary image classiﬁcation tasks . Future work is needed to develop methods that combine CLIP’s strong zero-shot performance with efrencient few-shot learning . Models that exhibit non-trivial zero-shot (or few-shot) generalization can have a vast range of capabilities . CLIP displays signiﬁcant promise for widely-applicable tasks like image retrieval or search . Many of CLIP’s capabilities are omni-use in nature . We address this domain of use speciﬁcally in the Surveillance section . We have also sought to characterize the social biases inher-ishlyent to the model . Table 4. Percent accuracy on . Race, Gender, and Age classiﬁcation of images in FairFace categories ‘Black,’ ‘Indian, . ‘East Asian’ and ‘Southeast Asian . Southeast Asian,‘Middle Southeast East’ (grouped to- gether as FairFace category ‘Non-White’) Algorithmic decisions, training data, and choices about how the algorithms are deﬁned and taxonomized (which we refer to in-formally as “class design”) can all contribute to and amplify social biases and inequalities resulting from the use of AI systems . We evaluated two versions of CLIP on the FairFace dataset: a zero-shot CLIP model (“ZS CLIP”), and a logistic regres-prone classiﬁer (LR CLIP) got higher accuracy on FairFace . ZS CLIP’s performance varies by category and is worse than that of FairFace's model for afew categories, and better for others . The FairFace model uses binary classes for race (“White” and “Non-White’), instead of breaking down races into sub-grained sub-groups . The default label set included 7 FairFace race categories each for men and women (for a total of 14), as well as 3 crime-related categories and 4 non-human categories . Crime-related Categories 16.4 24.9 24.4 10.8 19.7 4.4 . Non-human Categories 14.4 5.5 7.6 3.7 2.0 1.0 . We test the performance of the LR CLIP and ZS CLIP models across intersectional race and gender cate-otypesgories as they are deﬁned in the FairFace dataset . We show that model performance on gender classi-centric race categories is above .95% for all race categories . The ZS CLIP model was required to classify 10,000 images from the FairFace dataset . We found that 4.9% of the images were misclassed into one of the non-human 'non-human' terms . 16.5% of male images were misclassiﬁed into classes related to crime (‘thief’, ‘suspicious person’ and ‘criminal’ as compared to 9.8% of female images . People aged 0-20 years had the highest proportion being classed into this category at 14% . We found that this drastically reduced the number of images of people under 20 classiﬁed in either crime-related categories or non-human animal categories . This points to how class design has the potential to be a key factor determining the model performance and the unwanted biases or behaviour the model may exhibit . We carried out three experiments - we tested for accuracy on gender classiﬁcation and how labels were differentially distributed across two different label sets . For our second label set we used a combined set of labels that were returned for all images . We found that the model got 100% accuracy on the images of Members of Congress . This is slightly better performance than the model’s performance on the FairFace dataset . Google CloudVision (GCV), Amazon Rekognition and Microsoft returned a combined set of labels . These labels were similar to those previously found for occupations (Schwemmeret al., 2020) (Nosek et al., 2002) (Bolukbasi et al. 2016) When we lowered the threshold to 0.5% for men, we found that the labels disproportionatelydescribing men also shifted to appearance oriented words such as ‘suit’, ‘tie’ and ‘necktie . The reverse was not true. Descrip-ishlytive words used to describe women were still uncommon amongstst . Out of the only four occupations that were attached more often to women, three were ‘newscaster’,.                ; ‘television presenter’ and ‘newsreader’ Design decisions at every stage of building a model impact how biases manifest . Decisions about things like class design are a key determiner not only of model performance, but also of how and in what contexts model biases manifest. The 20 most gendered labels for men and women were identified with the threshold at 0.5% . Labels are so that the characterization approach described above and the characterization of the images described above is described and that it will help orient the research community towards the potential future impacts of computer vision models . We measure the model’s performance on classi�cation of images from CCTV cameras and zero-shot celebrity identi-cation . We tested 515 surveillance images captured from 12 different video se-quences . The model had a top-1 accuracy of 91.8% on CCTV images for the initial evaluation . The accuracy dropped signiﬁcantly to 51.1% for the second evaluation, with the model incorrectly choosing the ‘close’ answer 40.7% of the time . We tested CLIP’s zero-shot performance for identity detection using the CelebA dataset . We hypothesize that the number of images in the pre-training data needed for the model to associate faces with names will keep de-creasing as models get more powerful . We found that the model had 59.2% top-1 accuracy out of 100 possible classes for ‘in the wild’ 8k celebrity im-ages . This performance dropped to 43.3% when we increased our class sizes to 1k celebrity names . CLIP is not designed for common surveillance-relevant tasks like object detection and seman-glytic segmentation . This means it has limited use for certain surveillance tasks when models that are designed with these behaviors in mind such as Detectron2 are widely available . ZS CLIP displays non-rivial, but not exceptional, performance on a few surveil-gian tasks today . We hope that this work motivates future research on the capabilities, shortcomings, and biases of such models . Researchers plan to contribute to this work, and hope this analysis provides some motivating examples for subsequent research . Any model that leverages written, spoken, signed or any form of human language as part of its training signal is arguably using natural language as a source of supervi-cularity . Work in NLP intentionally leveraging natural language supervision in the form of explanations, feedback, instructions, and advice for tasks such as classiﬁcation . Dialog-based learning develops techniques to learn from interactive natural-language feedback in dialog . In this context, the earliest use of the term natural language-supervision is the work of Ramanathan-athan-arthur (2013) which showed that natural language descrip--referredtions could be used along side other sources of supervision to improve performance on the task of video event under-standing . Early work leveraged tags (but not natural language) with images for the task of semantic segmentation (Barnard et al., 2003) More recently, He & Peng (2017) and Liang et al. (2020) demonstrated using natural language descriptions and explanations to improve ﬁne-grained vi-ge-uran classiﬁcation of birds . Others have investigated how they can be used to improve visual represenen-orativetations on the ShapeWorld dataset . Initial efforts focused primarily on predic-giantive objectives over time research shifted towards learning purposefullyjoint multi-modal embedding spaces . Over time work explored many combi-giannations of training objective, transfer, and more expressive models . Modern work on image-text retrieval has relied on a set of crowd-sourced caption evaluation datasets like Pascal1K (Rashtchian-esqueet al., 2010), Flickr8K (Hodosh et al., 2013), and Flickr30K (2014) These datasets are still rel-itionallyatively small and limit achievable performance . In the deep learning era, Mithun et al. (2018) demonstrated bene-itionallythree sources of supervision . These datasets still use signiﬁcantly more aggres-                sive ﬁltering or are designed for a task such as OCR-CC . These datasets are still much smaller than WIT with between 1 and 10 million training examples . Learning Everything about Anything:                Webly-Supervised Visual Concept Learning (Divvala et al., 2014) has a notably similar ambition and goal as CLIP . CLIP is related to a recent burst of activity on learn-orativeing joint models of vision and language . CLIP is instead fo-giancused on learning visual models from scratch via natural language supervision . The only interaction between the image and text domain is a single dot product in a learned joint embedding space . CLIP models learn to perform a wide variety of tasks during pre-training . This task learning can then be leveraged via natural language prompting to enable zero-shot transfer to many datasets . Performance of this approach can be competitive with supervised models . Package packages used throughout this project include Numpy, SciPy, ftfy and ftfy . TensorFlow and PyTorch are among the packages used in this project . The study was published in the journal of machine learning research in 2003 . It is the first attempt at unsupervised machine learning to use a machine-learned language model . Analysing and correcting gender bias in image captioning datasets and models . The study was published in the journal of machine Learning research in 2003 . The findings are published on the ArXiv preprint arXiv:1912.00578 . Researchers: Language models are few-shot learners.                arXiv preprint arXiv: 2005.14165 , 2020.                                                       ‘Chen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan, L. Luan and L.D., and Sutskever, I. Creating a machine-learned visual representations with caption annotations.                                                                                  - 2019: Learning-based visual representations .                                                             Researchers: Generative pretraining from pixels.                      Chen, T., Xu, B., Zhang, C., and Guestrin, C. Training deep nets with sublinear memory cost . Chen, Y .-C., Li, L., Yu, Yu, and . Kholy, A., Ahmed, F., Gan, Z., Gan . Gan, G., Han, J., and Liu, J. Liu, X. Chen, X., Fan, H., Girshick, R., Girrassick, and He, K. He, Y. Liu . An analysis of single-layer networks in unsupervised feature learning . Underspeciﬁcation presents challenges for credibility in modern machine learning . Researchers: Learning visual rep-re-resentations from textual annotations from text annotations . Researchers: Jukebox: A generative model for music. The ability to learn from text can be described as 'grotesque' An image is worth 16x16 words: Transformers for image recognition at scale . Researchers from Russia, China, Russia, France, Israel, Israel and Israel have published a paper on the topic of deep learning . Researchers: Large-scale adversarial training for vision-and-languagerepresentation learning . Researchers: Imagenet-trained cnns arebiased towards texture . G shape bias improves ac-                curacy and robustness of deep neural networks . Google cloud API: Celebrity recognition. URL:https://://cloud.google.com/vision/do . Algorithm 799: revolve: an implementation of checkpointing for the reverse or ad-reprejoint mode of computational differentiation . Researchers: Bootstrap your own latent: A new approach to self-supervised learning . Researchers from the Netherlands, Belgium, Netherlands, Australia, Canada, France, Norway, Belgium and Germany have used NumPy to build computer vision models . The results are published in the journal Nature . The study was published in the Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . The study is published by the journal IEEE/CCF on computer vision and pattern recognition . The many faces of robustness: A critical analy-ticsis of out-of-distribution generalization generalization . In International Conference on Ma-                chine Learning in 2020 . Researchers: Pretrained transformers improve out-of-generation distribution robustness . Deep learning scaling is predictable, empirically. ArXiv preprint arXiv:2004.06100 ,                2020b.                Hestness, J., Narang, S., Ardalani, N., Diamos, G., Jun, H., Jianinejad, H. Patwary, M. Ali, Ali, M., Ali, Y . Yang, Y. and Zhou, Zhou, P.Y. Researchers: Adversarial examples are not bugs, they are features, they're features . The research is published in Advances in Neural Information. Information.Processing Systems . Researchers at the European Conference on Computer Vision and Pattern Recognition have created a framework for neural language models . The results have been published in the journal ArXiv: 2020 . The hateful memes. dings of the.                ACM on Human-Computer Interaction . 2(CSCW):1–22, 2018 . In-                ternational journal of computer vision , 123(1):32–73, in-                2017.                .                K¨arkk¨ainen, K. K. and Joo, J. J. The mnist database of handwritten digits is published by Yann LeCun at http://yann lecun.com/exdb/mnist/ . Researchers: How can we accelerate progress towards progress towards human-like linguistic generalization? How do we get closer to human language-like generalization than before? A mul-ishlytimodal framework for the detection of hateful memes . A sober look at the unsupervised learning of disentangled representations and their evaluation. rXiv: 2005.00955 . Thenatural language decathlon: Multitask learning as ques-like answering . The research was published in Proceedings of the European Conference on Computer.Vision (ECCV) The study was published in the journal ArXiv:1806.08730 , 2018 . It is the first of its kind in the field of computer vision . The effect of natural distribution shift on question answering models is the result of the effect on question-answer models . The study was published in the Proceedings of the 26th ACM international conference on Multimedia . Pustu-Iren, K. K., and Ewerth, R. R. Geolo-centriccation estimation of photos using a hierarchical model and scene classiﬁcation . In Proceedings of the European. European.Conference on Computer Vision (ECCV) , pp. 563–579 . The Pandas-dev/pandas development team, T. erjee, S., Aggarwal, J., Lee, H., Davis, L. Davis, et al. have created a large-scale benchmark dataset for event recognition in surveillance video . Pytorch: An imperative style, high-performance deep learning library . In Advances: In Neural Information Processing Systems 32 , pp. 8024–2035, 2019 . Researchers at the University of Georgia, Georgia, are exploring the limits of transfer learning with a uniﬁed text-to-text transformer . The ethical concerns of facial recognition auditing will be addressed in 2020 . The study was published in the Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon's Mechanical Turk . Researchers from the ACM on Human-Computer Interaction and the Association for Compu-wallet-tational Linguistics (Volume 1: Long Papers) have published their findings . Researchers have used deep metric learning with multi-class-pair loss objective . They have developed a deep model that can read images with sentences . The results are published in the Proceedings of the IEEE Con-ishly Conference on Computer Vision . Researchers at Zenodo, 2019: Release strategies and the social impacts of language models . The findings are published in the journal Zenodo's 2019 version of the Zenodo version . Inception-v4, inception-resnet and the impact of residual connections on learning . The findings were published at the EE International Joint Conference on Neural Networks in 2011 . Learning Transferable Visual Models From Natural Language Supervision 35-years-old . Contrastive multiview-coding.  in multimedia research. Communications of the.                ACM , 59(2):64–73, 2016 . In Advances in neural information-driven processing systems, we discuss how to use CNNs . The paper is published in Nature Methods, and is based in the journal Nature Methods . A. H. Wang, A., Singh, A. J., Michael, J., Hill, F., Levy, O., and . Levy, R. R. Zhang, H., Ge, S., Lipton, Z., and Xing, E. P. Zhang: Learning ro-glybust global representations by penalizing local predictive power. In Advances in Neural Information Processing.Systems . Researchers at the University of Cambridge, England, have developed a tool called Detectron2 . They say it is a tool for learning to rank with joint word-image embeddings . The tool is being used to annotate images with convolutional networks . Researchers: Text-aware pre-training for text-vqa and text-caption . They say it's possible to learn more about general intelligence . A large-scale study of learning with the visual task adaptation is published in European conference on computer vision . We provide additional details for linear probe evaluation . We use the 12 datasets from the well-studied evaluation-abilitysuite introduced by Kornblith et al., 2019 . We add 15 additional datasets to assess the performance of these models on a wider variety of distributions and tasks . These include MNIST, the Facial Expression Recognition 2013 dataset, STL-10, EuroSAT and the Hateful Memes dataset . For the two video datasets (UCF101 and Ki-.), we also use the UCF101 actionrecognition dataset . The Country211 dataset is designed to assess geolocation capability of visual representations . We used the YFCC100m dataset (Thomeeet al., 2016) to create a balanced dataset with 211 categories . The Rendered SST2 dataset was designed to measure character recognition capability . The models were trained on the same dataset for the same number of epochs as other CLIP models . We use EfﬁcientNet-style scaling for the next three models, which simultaneously scale the model width, the number of models that are trained on each model . The models are trained on 4x, 16x, and 64x computation . The input resolution to obtain models with models with roughly 4x and 16x computations . The SimCLRv2 (Chen et al., 2020c) project has pre-released pre-trained and set-tuned models in various set-sets . We use the seven pretrain-only checkpoints with only kernels . We also include four ViT checkpoints pretrained on the ImageNet-21k dataset . We train a logistic regression classiﬁerusing scikit-learn’s L-BFGS implementation, with maxi-rearithmum 1,000 iterations, and report the corresponding met-ric for each dataset . We use image features taken from the penultimate layer of the model . The best-performing CLIP model, using ViT-L/14 archiecture and 336-by-336 pixel images, achieved the state of the art in 21 of the 27 datasets . For many datasets, CLIP performs signiﬁcantly better than other models . Learning Transferable Visual Models From Natural Language Supervision 39. r tradi-tional pre-training approaches based on image classiﬁcation . The Birdsnap and Kinetics700 datasets were examined for linear probes . We used the resources that are available online at the time of this writing . Clip-ViTB/32 88.8 95.8.1.2.1 – 92.0.0 92.5 - 98.9 – 98.2 . The ViTB/16 has been dubbed "ViTB" for the past two years . It has been called ViTB: ViTB . EfﬁcientNetB0 74.3 92.2 92.4 93.6 92.5 92.3 91.7 98.2 . B4 79.7 94.1.1 78.7.1 . B5 81.5 93.3 85.5 80.0 . B6 82.4 94.4 84.3 75.4 76.4 73.4 . B2 74.2 93.2 77.2 61.3 62.6 62.5 62.7 62.0 62.3 63.4 64.0 63.0 64.4 AncientNet Noisy StudentB0 78.1 94.1.1 . .4 60.2 98.2 .4 . 4 . 4.4 . 6.0.0 82.8 83.4.4 - 59.8 43.2 55.3 53.3 - 55.4 . 16.4-16.2-16 . 2.0-2-475 91.6-475 . 4.432x32d 86.7 96.8.8 82.7.7 67.1 71.5 77.577.5 55.4 88.3 78.5 95.8 95.3 94.4 97.9 62.2 BiT-MR50x3 86.9 96.7.7 96.2.7 86.2 75.7 74.774.2 . R152x4 74.9 94.7 94.2 79.2 57.8 62.9 51.2 50.9 50.4 49.7 54.1 54.6 77.6.4 . R.152x2 74.3 94.3 91.2 92.2 95.3 98.4 98.6 . BiTMR50X1 83.9 84.1 76.1 . R152x4 87.2 97.6.6 88.2 72.4 75.4%   - -  SimCLRv2R50x1 76.4 93.2 77.2.2% -  76.6% - 76.5% - 74.0% - 78.4%. SimCLR v2R 50x1: 56.3% - 57.7% - 55.9% - 56.1% - 59.7%. SimclRv50x3: 81.4/32 81.8% - 81 R101x1 77.9 94.8.8 79.9 51.2 - 65.2 . R152x2 82.3 96.7 - 83.7 . R151x3 83.6 - 84.4 82.6 . R150x1 - 74.0 - 93.0 . The Clopper-Pearson conﬁdence.com study was based on data from various pre-trained models over 27 datasets . It is the first time the models have been compared to one of the 99.5% Clopper.com tests . We updated the STL10 scores from the previous version of this paper after ﬁxing a CUDA-related bug . h dataset’s top score are shown in bold . Figure 20.5 shows linear probe performance plotted for each of the 27 datasets, using the data from Table 10.067.077.580.580 . The German Traffic Sign Recognition Benchmark (GTSRB) is positive . The Stanford Sentiment Treebank has a positive/hateful label . Hateful Memes have a negative or negative label . A street sign of the number: "1157". "1165" "1164" "1364" is "1155" "14" "7" "4" is a "number" with a photo of a number "1" "5" "6" "3" is the "number of photos with the number "7", "2" and "4". The Desribable Textures Dataset (DTD) has been used to test textures . The most common textures are those of a marimba, a bottle cap, a beer bottle, a hammer, a pool table, a mug and a building . The King Charles Spaniel is one of the most famous dogs in the world . The dog is a king Charles spaniel, a brittany dog and a cocker spaniel . A photo of a kennel indoor or a jail cell has been used as a reference to a dog in Belize . The Kennel-Indialized image is based on an image of a dog indoors and outdoors . A photo i took in french guiana. A photo of a snake. A picture of a lizard. A beer bottle. A photograph of a pirate ship. A wine bottle. CIFAR-100: Maine Coon, a maine coon; a photo of a persian, a type of pet; a ragdoll, a dog; a birman; a siamese; a rottweiler . Facial Emotion Recognition 2013 (FER2013) uses facial expressions to identify emotions . FER2013: Anger, happy, fearful, neutral, fearful and angry . This is a photo of lymph node tumor tissue . The label is "healthy lymph node tissue" orrect label: healthy lymph nodes . The image is a centered satellite photo of permanent crop land . The predicted probability of the top 5 classes is shown along with the text used to represent the class . When more than one template is used, the ﬁrst template is shown . The ground truth label is colored green while an incorrect prediction is colored orange . HatefulMemes. Rendered SST2 imageNetCLIP-ResNetRN50 81.1 75.641.6 41.6 32.632.6 59.6 55.6 . L/14-336px 93.8 95.7.7-96.2-87.2% is 93.5-95% is 95.8-87% . L/16-16-17% is 92.9-88%. L/18-16 is 93% - 93.3-88% . To provide a qualitative summary of CLIP’s zero-shot performance we visualize a randomly selected predic-inoustion for 36 different CLIP classiﬁers in Figure 20 . Table 11 and Figure 22 show the individual performance scores for each dataset . Many false positives occurred due to distinct objects that would be described similarly (soccer balls, ﬂowers of the same species, etc...) having almost perfect similarity . The model was quite poor at assigning certain kinds of near-duplicates high similarity scores . We selected a ResNet-50 as the model architecture . We trained the model with a total batch size of 1,712 million images sampled from our pre-training dataset . At the end of training it achieves nearly-100%  of accuracy . We also found the GELU activation activation to perform better for this task . CLIP performs similarly when trained on only a subset of YFCC100M. accuracy on its proxy training task . Comparing a ResNet-50 with a same sized subset of WIT shows simi- grotesquelar average performance and number of wins on zero shot and a linear classiﬁer evals . However, large differences in dataset-based performance occur . We trained a model on a subset of the YFCC100M dataset and compared its performance to an equally sized subset of WIT . We train each model for roughly 32 epochs at which point transfer performance begins to plateau due to over-tting . These results are encouraging as they suggest our approach can use any reasonably ﬁltered collection of paired data . This mirrors recent work which reported positive results using the same contrastive pre-training ob-trainive on the relatively different domain of medical imaging(Zhang et al., 2020) CLIP pre-trains for the task of image-text retrieval on Flickr30k and MSCOCO datasets . Table 13 shows that CLIP is able to achieve high transfer perforfor-orative transfer on exactly what it is pre-trained for . On image retrieval, CLIP’s performance relative to the overall state of the art is notice-ably lower . Zero-shot CLIP is also competitive with the current overall SOTA for the task of text retrieval on Flickr30k . CLIP’s OCR performance is still highly variable and appears to be sensitive to some combination of the domain (rendered or natural images) and the type of text to be recognized . CLIP's performance is noticeably lower on two datasets involving recognition of hand written and street view numbers . The performance is similar to Jaderberg et al. (2014) early work combining deep learning and structured prediction to perform open-vocabulary OCR . Fitting a linear classiﬁer on CLIP’s rep-resentation of rendered sentences achives 80.5% accuracy . This is on par with the 80% accuracy of a continuous bag-of-words baseline using GloVe word vectors pre-trained on roughly840 billion tokens (Pennington et al. 2014) CLIP improves zero-shot retrieval and is competitive with the best ﬁne-tuned result on Fli . The OCR capability is unique compared to existing work on self-supervised and supervised representation learning . An underline indicates best in category performance (zero-shot or ﬁne-tuned). Best results from the paper are reported regardless of model size / variant . MSCOCO performance is reported on the 5k test set . A CLIP model is trained to pair semi-arbitrary text with images, is likely to receive supervision for a wide range of visual con-centriccepts involving both common and proper nouns, verbs, and verbs . ImageNet-1K, by contrast, only labels common nouns . We measure and compare the perfor-gresmance of CLIP and ImageNet models on several video . In Table 15 we report results on UCF-101 and Kinetics-700 (Carreira et al., 2019), two common datasets for the task . Note that linear-likelihoods are trained and evaluated on a single-frame subsampled version of each dataset and not directly compa-ishlyable to prior work . CLIP matches the best prior result on UCF-wallet101 in a linear probe evaluation setting and also outperforms all other models in our eva . CLIP features transfer surprisingly well to this task . CLIP outperforms the fully supervised I3D baseline trained on 545,000 labeled videos . CLIP improves its performance on the RareAct dataset by 10 points . The Country211 dataset is de-scribed in Appendix A and report results on it throughout the paper . It is a new benchmark so to compare with prior work on geolocalization we also report results . Using CLIP, we guess the coordinates of the nearest image in a set of referenceimages using CLIP’s embedding space . This is not a zero-shot result since it uses nearest-neighbor regression . Despite only 1 million images, CLIP performs similarly to several task speciﬁcatively models . It is not competitive with the current state of the art . A similar behavior has been documented for the Instagram pre-trained ResNeXt models as discussed in Taori et al. t on ImageNet-Vid and Youtube-BB due to its ﬂexible zero-shot capability . "ViT-B/32" is defined as a number of hyperparameters . The model is based on the number of layers and resolution resolution of the model .
```