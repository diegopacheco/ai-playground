### What is optimization?

* Optimizing its(model) parameters on our data(training data).
* Training a model is an iterative process
* Each iteration the model makes a guess about the output, calculates the error in its guess (loss)
* Collects the derivatives of the error with respect to its parameters 
* Optimizes these parameters using gradient descent
* Back propagation explained: https://www.youtube.com/watch?v=tIeHLnjs5U8