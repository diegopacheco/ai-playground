{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative AI\n",
    "\n",
    "Gen AI is a new field of AI that focuses on creating new content, such as images, music, and text. \n",
    "\n",
    "<img src=\"images/timeline-genai.png\" width=\"600\" height=\"400\" />\n",
    "\n",
    "## Gen AI Use Cases\n",
    "\n",
    "**Text Generation**: Create new text, such as articles, stories, or conversations.\n",
    "\n",
    "**Image Generation**: Create new images, such as photos, artwork, or designs.\n",
    "\n",
    "**Music Generation**: Compose music, melodies, or sound effects.\n",
    "\n",
    "**Video Generation**: Create new videos, such as animations or clips.\n",
    "\n",
    "**Data Generation**: Create synthetic data for training or testing.\n",
    "\n",
    "**Style Transfer**: Transfer styles between images, music, or text.\n",
    "\n",
    "**Image-to-Image Translation**: Translate images from one domain to another.\n",
    "\n",
    "**Text-to-Image Synthesis**: Generate images from text descriptions.\n",
    "\n",
    "**Dialogue Generation**: Engage in conversation, responding to user input.\n",
    "\n",
    "**Creative Writing**: Generate creative writing, such as poetry or short stories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning\n",
    "\n",
    "Deep learning is a subfield of machine learning that focuses on neural networks.\n",
    "\n",
    "Neural Networks are a type of model that is inspired by the human brain. They consist of layers of neurons that process input data and produce output data. Deep Neural Networks are neural networks with many layers. They are capable of learning complex patterns in data.\n",
    "\n",
    "<img src=\"images/nn.png\" width=\"600\" height=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks (RNNs)\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are a type of neural network that is designed to process sequences of data. They are commonly used for text and speech processing.\n",
    "\n",
    "<img src=\"images/RNN.png\" width=\"600\" height=\"400\" style=\" background-color: white;\" />\n",
    "\n",
    "**Backward Propagation**: RNNs use backpropagation to update their weights and biases during training. This involves computing the gradient of the loss function with respect to the weights and biases of the network.\n",
    "\n",
    "**Foward Propagation**: RNNs use forward propagation to compute the output of the network given an input sequence. This involves passing the input sequence through the network and computing the output at each time step.\n",
    "\n",
    "**Long Short-Term Memory (LSTM)**: LSTMs are a type of RNN that are designed to capture long-term dependencies in data. They are capable of learning patterns that are separated by long sequences of data.\n",
    "\n",
    "Use Cases:\n",
    " * Natural Language Processing (NLP)\n",
    " * Speech Recognition\n",
    " * Time Series Prediction / Anomaly Detection\n",
    " * Music Composition\n",
    " * Handwriting Recognition\n",
    " * Video Activity Recognition\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks (CNNs)\n",
    "\n",
    "Convolutional Neural Networks (CNNs) are a type of neural network that is designed to process images. They are commonly used for image recognition and computer vision.\n",
    "\n",
    "<img src=\"images/CNN.jpeg\" width=\"600\" height=\"400\" style=\" background-color: white;\" />\n",
    "\n",
    "CNNs use convolutional layers to extract features from images. These layers apply filters to the input image to detect patterns, such as edges, corners, and textures.\n",
    "\n",
    "Use Cases:\n",
    "* Image Classification\n",
    "* Object Detection\n",
    "* Image Segmentation\n",
    "* Face Recognition\n",
    "* Medical Imaging\n",
    "* Gesture Recognition\n",
    "* Image Synthesis\n",
    "* Self-Driving Cars\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks Zoo\n",
    "\n",
    "The Neural Network Zoo is a collection of neural network architectures that have been developed over the years. It includes a wide range of models, such as convolutional neural networks (CNNs), recurrent neural networks (RNNs), and transformers. The Neural Network Zoo is a useful resource for researchers and practitioners who want to explore different neural network architectures.\n",
    "\n",
    "<img src=\"images/nn-zoo.png\" width=\"800\" height=\"1024\" />\n",
    "\n",
    "Source: https://www.asimovinstitute.org/neural-network-zoo/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers\n",
    "\n",
    "Transformers are a type of neural network architecture that has been successful in many Gen AI tasks. They are based on self-attention mechanisms that allow them to model long-range dependencies in data. Transformers have been used in many applications, such as language modeling, translation, and image generation.\n",
    "\n",
    "<img src=\"images/transformers.png\" width=\"600\" height=\"600\" />\n",
    "\n",
    "Use Cases:\n",
    "* Natural Language Processing (NLP)\n",
    "* Language Modeling\n",
    "* Question Answering\n",
    "* Speech Recognition\n",
    "* Time Series Forecasting\n",
    "* Image Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "Embeddings are a way to represent data in a lower-dimensional space. They are commonly used in Gen AI to represent words, images, and other types of data. Embeddings are learned during training and capture the relationships between different data points.\n",
    "\n",
    "<img src=\"images/Transformers-attention-1.png\" />\n",
    "\n",
    "Using cosine similarity to find similar embeddings.\n",
    "\n",
    "## Embedding Code Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diego/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1290,  0.0631,  0.0218,  ..., -0.2822,  0.3723,  0.4550],\n",
      "         [ 0.7875, -0.0711, -0.3430,  ...,  0.0156,  0.5159,  0.1851],\n",
      "         [ 1.1745,  0.9952,  0.3751,  ...,  0.0757,  0.2636, -0.0634],\n",
      "         ...,\n",
      "         [-0.1707,  0.2353, -0.0354,  ..., -0.6519, -0.3293,  0.1394],\n",
      "         [ 0.1527, -0.3066,  0.0453,  ...,  0.2441,  0.2385, -0.5898],\n",
      "         [ 0.4515,  0.0422,  0.0441,  ...,  0.2707, -0.4147, -0.2581]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Define a sentence\n",
    "sentence = \"I love coding Gen AI applications.\"\n",
    "\n",
    "# Tokenize the sentence and obtain the input IDs\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "# Get the embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# The last hidden state is the first element of the output tuple\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "print(last_hidden_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a Tensor?\n",
    "\n",
    "* key data structure used in creating, training, and testing neural networks\n",
    "* Generalization of vectors and matrices\n",
    "* Object represented as arrays of numbers\n",
    "* Tensor Ranks:\n",
    "  * 0: Scalar\n",
    "  * 1: Vector\n",
    "  * 2: Matrix (2D array)\n",
    "  * 3: 3D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scalar (Rank 0 tensor):\n",
      " 5\n",
      "\n",
      "Vector (Rank 1 tensor):\n",
      " [1 2 3]\n",
      "\n",
      "Matrix (Rank 2 tensor):\n",
      " [[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "\n",
      "Rank 3 tensor:\n",
      " [[[ 1  2  3]\n",
      "  [ 4  5  6]\n",
      "  [ 7  8  9]]\n",
      "\n",
      " [[10 11 12]\n",
      "  [13 14 15]\n",
      "  [16 17 18]]\n",
      "\n",
      " [[19 20 21]\n",
      "  [22 23 24]\n",
      "  [25 26 27]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# A scalar (rank 0 tensor)\n",
    "scalar = np.array(5)\n",
    "print(\"Scalar (Rank 0 tensor):\\n\", scalar)\n",
    "\n",
    "# A vector (rank 1 tensor)\n",
    "vector = np.array([1, 2, 3])\n",
    "print(\"\\nVector (Rank 1 tensor):\\n\", vector)\n",
    "\n",
    "# A matrix (rank 2 tensor)\n",
    "matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "print(\"\\nMatrix (Rank 2 tensor):\\n\", matrix)\n",
    "\n",
    "# A rank 3 tensor\n",
    "tensor = np.array([[[1, 2, 3], [4, 5, 6], [7, 8, 9]],\n",
    "                   [[10, 11, 12], [13, 14, 15], [16, 17, 18]],\n",
    "                   [[19, 20, 21], [22, 23, 24], [25, 26, 27]]])\n",
    "print(\"\\nRank 3 tensor:\\n\", tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Database\n",
    "\n",
    "A vector database is a database that stores vectors in a high-dimensional space. It allows you to query vectors based on their similarity to other vectors. This is useful for many Gen AI tasks, such as image search, recommendation systems, and content generation.\n",
    "\n",
    "<img src=\"images/vector_databases.png\" width=\"600\" height=\"400\" />\n",
    "\n",
    "Popular Vector Databases:\n",
    " * LanceDB\n",
    " * Pinecone\n",
    " * Chroma\n",
    " * OpenSearch\n",
    " * Redis\n",
    " * Postgres (pg_vector)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large Language Models\n",
    "\n",
    "Large language models are a type of Gen AI model that is trained on a large corpus of text data. They are capable of generating human-like text and engaging in conversation. Large language models have been used in many applications, such as chatbots, content generation, and translation.\n",
    "\n",
    "A lot of people see LLM as a chatbot, but it is more than that. \n",
    "\n",
    "<img src=\"images/cb-hero-top.png\" width=\"300\" height=\"400\" style=\" background-color: white;\" />\n",
    "\n",
    "A better metaphor it would see LLM as a Operational System (OS).\n",
    "\n",
    "<img src=\"images/llm-os.png\" width=\"600\" height=\"400\" style=\" background-color: white;\" />\n",
    "\n",
    "[GPT-4o](https://openai.com/index/hello-gpt-4o/) and Google products with GenAI - [GoogleIO/24](https://www.tomsguide.com/news/live/google-io-2024-keynote) are going exatclyt into this direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatGPT\n",
    "\n",
    "GPT means: Generative Pre-trained Transformer.\n",
    "\n",
    "Poster child for LLMs and Transformers Architecture.\n",
    "\n",
    "ChatGPT is a large language model (LLM) that is based on the GPT-series. It is capable of generating human-like text responses to user input. ChatGPT has been used in many applications, such as chatbots, conversational agents, and creative writing.\n",
    "\n",
    "<img src=\"images/GPT_VS_BERT.png\" />\n",
    "\n",
    "### GPT Cost to train\n",
    "\n",
    "| Model    | Parameters | Cost   |  Time    |\n",
    "| -------- | ---------- |--------|----------| \n",
    "| GPT-3    | 175B       | 4.6M   | 34 days  |\n",
    "| GPT-4    | 100T       | 2.6B   | 100 days |\n",
    "| LLAMA-2  | 70B        | 20M    | 23 days  |\n",
    "\n",
    "Llama-2 paper, it took 184,320 GPU hours of an A100 to train the model. 184320 hours = 7680 days ~= 21 years Renting AWS p4d. 24xlarge instance (8 GPUs) is $32.7726 per hour\n",
    "\n",
    "Cost References:\n",
    "* [GPT-4 Training days](https://towardsdatascience.com/the-carbon-footprint-of-gpt-4-d6c676eb21ae#:~:text=Recall%20that%20it's%20estimated%20that,to%202%2C600%20hours%20per%20server.)\n",
    "* [GPT-3 Cost](https://www.forbes.com/sites/craigsmith/2023/09/08/what-large-models-cost-you--there-is-no-free-ai-lunch/?sh=3561d9e24af7)\n",
    "* [GPT-3 Cost comments](https://ai.stackexchange.com/questions/43128/what-is-accelerated-years-in-describing-the-amount-of-the-training-time#:~:text=Since%20GPT%2D3%20is%20a,train%20the%20GPT%2D3%20model.)\n",
    "* [GPT Cost](https://www.moomoo.com/community/feed/109834449715205#:~:text=In%20terms%20of%20the%20training,reach%2046%20million%20U.S.%20dollars.)\n",
    "* [LLAMA 2 model card](https://github.com/meta-llama/llama/blob/main/MODEL_CARD.md)\n",
    "* [LLAMA 2 Cost](https://www.quora.com/How-much-money-did-Meta-cost-to-train-Llama-2-Why-has-NVIDIAs-stock-price-been-rising-and-how-high-could-it-ultimately-go#:~:text=Cost%20Breakdown%3A%20Meta%20allocated%20a,the%20training%20of%20Llama%202.)\n",
    "* [LLAMA2 Cost comments](https://www.linkedin.com/posts/hherry_according-to-the-llama-v2-paper-it-took-activity-7128798861606694912-FLaT/?trk=public_profile_like_view)\n",
    "* [LLAMA2 training days](https://news.ycombinator.com/item?id=35008694)\n",
    "\n",
    "Cost of Train in AWS Sagemaker\n",
    "\n",
    "<img src=\"images/LLM-Cost-of-training-SageMaker-2023.png\" />\n",
    "\n",
    "Size of GPT-4 model\n",
    "\n",
    "<img src=\"images/GPT_4_metaphor.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Engineering\n",
    "\n",
    "Prompt engineering is a technique used to guide the output of a language model by providing a specific prompt. It involves designing prompts that encourage the model to generate desired responses. Prompt engineering is commonly used in chatbots, content generation, and translation.\n",
    "\n",
    "<img src=\"images/Prompt-Engineering-Example.png\" width=\"600\" height=\"400\" />\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approaches to prompt engineering\n",
    "\n",
    "**Zero-Shot Learning** The model is given a task without any examples at the beginning of the prompt. The model is expected to perform the task without any guidance. For instance, if you want the model to translate English to French, you might provide the English sentence you want translated without any examples of English sentences and their French translations. The idea is that the model should be able to perform the task without any examples.\n",
    "\n",
    "**Few-Shot Learning** The model is given a few examples of the task at the beginning of the prompt. These examples help the model understand the task it needs to perform. For instance, if you want the model to translate English to French, you might provide a few examples of English sentences and their French translations before providing the English sentence you want translated. The idea is that these examples guide the model towards the correct output.\n",
    "\n",
    "**Chain-of-Thought** The model is given a sequence of prompts that build on each other to perform a task. For instance, if you want the model to write a story, you might provide a sequence of prompts that guide the model towards the desired story. The idea is that each prompt builds on the previous one to create a coherent narrative.\n",
    "\n",
    "**Tree-of-Thought** The model is given a tree structure of prompts that guide the model towards the desired output. For instance, if you want the model to generate a dialogue, you might provide a tree structure of prompts that guide the model towards the desired conversation. The idea is that the tree structure helps the model navigate the space of possible outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face\n",
    "\n",
    "Hugging Face is a company that specializes in natural language processing (NLP) and Gen AI. They provide a wide range of models, datasets, and tools for developers to use in their projects. Hugging Face is known for their Transformers library, which is a popular open-source library for working with transformer models. Huggingface is like Github for AI models.\n",
    "\n",
    "### Hugging Face Tasks\n",
    "\n",
    "<img src=\"images/hf-tasks.png\" width=\"600\" height=\"400\" />\n",
    "\n",
    "https://huggingface.co/tasks\n",
    "\n",
    "It's the real deal! Makes easy to consume models and perform common AI tasks on text, audio, image or video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diego/.local/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "2024-05-15 02:46:15.143305: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-15 02:46:16.220799: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-05-15 02:46:16.220951: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-05-15 02:46:16.220965: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "/home/diego/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: POSITIVE, with score: 0.9988325238227844\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Initialize the Hugging Face sentiment-analysis pipeline\n",
    "nlp = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# Use the pipeline to analyze this sentence\n",
    "result = nlp(\"I love using Generative AI\")[0]\n",
    "\n",
    "# Print the result\n",
    "print(f\"label: {result['label']}, with score: {result['score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Model Evaluation Metrics\n",
    "\n",
    "**BLEU Score**: The BLEU score is a metric used to evaluate the quality of machine translation. It measures the overlap between the generated translation and the reference translation. The BLEU score ranges from 0 to 1, with higher scores indicating better translations.\n",
    "\n",
    "**ROUGE Score**: The ROUGE score is a metric used to evaluate the quality of text summarization. It measures the overlap between the generated summary and the reference summary. The ROUGE score ranges from 0 to 1, with higher scores indicating better summaries.\n",
    "\n",
    "**Perplexity**: Perplexity is a metric used to evaluate the quality of language models. It measures how well the model predicts the next word in a sequence. Lower perplexity scores indicate better language models.\n",
    "\n",
    "**F1 Score**: The F1 score is a metric used to evaluate the quality of classification models. It measures the balance between precision and recall. The F1 score ranges from 0 to 1, with higher scores indicating better classification models.\n",
    "\n",
    "**Accuracy**: Accuracy is a metric used to evaluate the quality of classification models. It measures the percentage of correctly classified examples. Higher accuracy scores indicate better classification models.\n",
    "\n",
    "##  LLM model evaluation benchmarks\n",
    "\n",
    "LLM Benchmarking is a way to evaluate the performance of large language models (LLMs) on a variety of tasks. It involves testing the model on different datasets and measuring its performance using various metrics. LLM benchmarking helps researchers and practitioners understand the strengths and weaknesses of different LLMs and compare them to each other.\n",
    "\n",
    "<img src=\"images/llm_leaderboard_sept_2023.png\" width=\"600\" height=\"400\" />\n",
    "\n",
    "Sample Benchmark example (from [Sep/2023](https://www.trustbit.tech/blog/2023/9/20/llm-performance-series-batching))\n",
    "\n",
    "**LLMU - Large Language Model Understanding**: is a new metric that measures the understanding of large language models (LLMs) in real-world applications. It takes into account factors such as interpretability, fairness, and bias. LLMU is designed to help researchers and practitioners evaluate the ethical implications of using LLMs in different contexts.\n",
    "\n",
    "**GLUE Benchmark**: GLUE (General Language Understanding Evaluation) benchmark provides a standardized set of diverse NLP tasks to evaluate the effectiveness of different language models\n",
    "\n",
    "**SuperGLUE Benchmark**: The Super General Language Understanding Evaluation (SuperGLUE) benchmark is an extension of the GLUE benchmark that includes more challenging tasks and requires models to perform better than human baselines. It includes tasks such as natural language inference, coreference resolution, and commonsense reasoning. SuperGLUE Benchmark Compares more challenging and diverse tasks with GLUE, with comprehensive human baselines\n",
    "\n",
    "**HellaSwag**: HellaSwag is a benchmark that evaluates the ability of language models to perform common-sense reasoning. It includes tasks such as predicting the next word in a sentence and completing a story with a plausible ending. Evaluates how well an LLM can complete a sentence.\n",
    "\n",
    "**TruthfulQA**: TruthfulQA is a benchmark that evaluates the ability of language models to generate truthful answers to questions. It includes tasks such as fact-checking and question-answering. Evaluates how well an LLM can generate truthful answers to questions.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ONNX - Open Neural Network Exchange\n",
    "\n",
    "ONNX is an open-source format for representing deep learning models. It allows models to be trained in one framework and deployed in another. ONNX is supported by many popular deep learning frameworks, such as PyTorch, TensorFlow, and MXNet.\n",
    "\n",
    "ONNX is a big deal because you can go from PyTorch <--> Tensorflow but also train a model in python and run inference on the model in Java.\n",
    "\n",
    "<img src=\"images/onnx.png\" width=\"600\" height=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain\n",
    "\n",
    "LangChain is a framework designed to simplify the creation of applications using large language models (LLMs). As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\n",
    "\n",
    "<img src=\"images/overall-langchain.jpeg\" width=\"600\" height=\"500\" />\n",
    "\n",
    "Java has [Langchain4J](https://docs.langchain4j.dev/) as langchain implementation for Java.\n",
    "\n",
    "Using Langchain we can do:\n",
    "* Smooth integration into your Java applications (or other languages), There is two-way integration between LLMs and Java: you can call LLMs from Java and allow LLMs to call your Java code in return.\n",
    "* Prompt Templating\n",
    "* Output parsing\n",
    "* Patterns like RaG or Agents\n",
    "* Vector Databases integration (Pinecone, OpenSearch, Redis, pg_vector, more...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RaG (Retrieval-augmented Generation)\n",
    "\n",
    "LLMs can halucinate.\n",
    "\n",
    "<img src=\"images/LLM_hallucinations.png\"  />\n",
    "\n",
    "RaG is approach that combines retrieval and generation models to improve the performance of language models. It uses a retrieval model to find relevant information from a large corpus of text and a generation model to generate responses based on that information.\n",
    "\n",
    "<img src=\"images/Rag-Simple.png\"  />\n",
    "\n",
    "RaG is key because it allows to generate more accurate and relevant responses by combining the strengths of retrieval and generation models. RaG also can reduce the cost of fine-tunning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stable Diffusion\n",
    "\n",
    "Stable Diffusion works by iteratively adding noise to an image to create a sequence of noisy images. The model then learns to denoise these images to recover the original image. This process is repeated multiple times to generate high-quality images.\n",
    "\n",
    "<img src=\"images/Stable_Diffusion_architecture.png\" />\n",
    "\n",
    "Stable Diffusion Common Models:\n",
    "* Stable Diffusion 3 (Stability AI)\n",
    "* DALL-E 3 (Open AI)\n",
    "* Imagine with Meta AI (Meta)\n",
    "* Midjourney\n",
    "\n",
    "Chalenges with Stable Diffusion:\n",
    "* Computational Intensity / High Hardware Demands\n",
    "* Quality Variance\n",
    "* Technical Complexity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Adversarial Networks (GAN)\n",
    "\n",
    "Generative Adversarial Networks (GANs) are a type of neural network that is designed to generate new data. They consist of two networks: a generator and a discriminator. The generator creates new data samples, while the discriminator tries to distinguish between real and fake samples.\n",
    "\n",
    "<img src=\"images/GANs.png\" style=\" background-color: white;\" />\n",
    "\n",
    "GAN Pros:\n",
    "  * Synthetic data generation\n",
    "  * High-quality results\n",
    "  * Versatility\n",
    "\n",
    "GAN Cons:\n",
    "  * Training Instability\n",
    "  * Computational Cost\n",
    "  * Overfitting\n",
    "  * Bias and Fairness\n",
    "  * Interpretability and Accountability\n",
    "\n",
    "Stable Diffusion vs GAN: The main difference between the two methods is their approach to generating new data. Stable Diffusion uses a process of adding and removing noise to an image, while GANs use a game-theoretic approach where two networks compete against each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling LLMs\n",
    "\n",
    "Large Language Models (LLMs) are fundamentally inefficient. They require a lot of computational resources to train and run. Scaling LLMs is a challenge. There are a lot of optimiations and improvements the community is working on to make LLMs more efficient.\n",
    "\n",
    "Common LLM architecture improvements:\n",
    "* Sparsity: Reduce the number of parameters in the model by using sparse matrices.\n",
    "* Quantization: Reduce the precision of the model's weights and activations to save memory.\n",
    "* Pruning: Remove unnecessary connections in the model to reduce the number of parameters.\n",
    "* Knowledge Distillation: Train a smaller model to mimic the behavior of a larger model.\n",
    "* Model Parallelism: Split the model across multiple devices to speed up training and inference. However the issue is that inreases latency.\n",
    "* Data Parallelism: Split the data across multiple devices to speed up training. However the issue is that inreases latency.\n",
    "* Mixed Precision Training: Use a combination of single and half precision to speed up training.\n",
    "* Gradient Checkpointing: Store intermediate activations to reduce memory usage.\n",
    "* Efficient Attention Mechanisms: Use more efficient attention mechanisms to reduce the computational cost of self-attention.\n",
    "* Efficient Transformers: Use more efficient transformer architectures to reduce the number of parameters in the model.\n",
    "* Efficient Embeddings: Use more efficient embeddings to reduce the size of the model.\n",
    "* Efficient Activation Functions: Use more efficient activation functions to reduce the computational cost of the model.\n",
    "* Efficient Loss Functions: Use more efficient loss functions to reduce the computational cost of the model.\n",
    "\n",
    "### Efficient Transformers\n",
    "\n",
    "**Flash Attention**: Attention operations have a memory bottleneck. Flash Attention is a more efficient attention mechanism that reduces the computational cost of self-attention. It uses a combination of local and global attention to capture long-range dependencies in data. Flash Attention is an attention algorithm used to reduce this problem and scale transformer-based models more efficiently, enabling faster training and inference.\n",
    "\n",
    "<img src=\"images/attention-vs-flash-attention.png\" />\n",
    "\n",
    "**Paged Attention**: PagedAttention attempts to optimize memory use by partitioning the KV cache into blocks that are accessed through a lookup table. Thus, the KV cache does not need to be stored in contiguous memory, and blocks are allocated as needed. The memory efficiency can increase GPU utilization on memory-bound workloads, so more inference batches can be supported.\n",
    "\n",
    "<img src=\"images/PagedAttentionKV.jpg\" />\n",
    "\n",
    "**Quantization**: Quantization ishow we can make LLM smaller. Using methods like GPTQ is a post-training quantization method to make the model smaller. It quantizes the layers by finding a compressed version of that weight, that will yield a minimum mean squared error.\n",
    "\n",
    "<img src=\"images/LLM-Quantization.png\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KAN\n",
    "\n",
    "[KAN](https://arxiv.org/abs/2404.19756) it's a potential Alternative to MLP.\n",
    "\n",
    "<img src=\"images/KAN-simple.png\" />\n",
    "\n",
    "KANs diverge from traditional Multi-Layer Perceptrons (MLPs) by replacing fixed activation functions with learnable functions, effectively eliminating the need for linear weight matrices.\n",
    "\n",
    "<img src=\"images/KAN_VS_MLP.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Security\n",
    "\n",
    "[OWASP Top 10 for LLMs](https://owasp.org/www-project-top-10-for-large-language-model-applications/)\n",
    "\n",
    "LLM01: Prompt Injection\n",
    " * crafty inputs, causing unintended actions by the LLM\n",
    "\n",
    "LLM02: Insecure Output Handling\n",
    " * XSS, CSRF, SSRF\n",
    "\n",
    "LLM03: Training Data Poisoning\n",
    " * LLM training data is tampered\n",
    "\n",
    "LLM04: Model Denial of Service\n",
    " * Attackers cause resource-heavy operations on LLMs\n",
    "\n",
    "LLM05: Supply Chain Vulnerabilities\n",
    " * third-party datasets, pre- trained models, and plugins can add vulnerabilities\n",
    "\n",
    "LLM06: Sensitive Information Disclosure \n",
    " * LLMs may inadvertently reveal confidential data in their responses\n",
    " * Unauthorized data access, privacy violations, and security breaches.\n",
    "\n",
    "LLM07: Insecure Plugin Design\n",
    " * LLM plugins can have insecure inputs and insufficient access control\n",
    "\n",
    "LLM08: Excessive Agency\n",
    " * Excessive functionality, permissions, or autonomy granted to the LLM-based systems\n",
    "\n",
    "LLM09: Overreliance\n",
    " * Misinformation, miscommunication, legal issues, and security vulnerabilities\n",
    "\n",
    "LLM10: Model Theft\n",
    " * Unauthorized access, copying, or exfiltration of proprietary LLM models\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
