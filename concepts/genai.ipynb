{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative AI\n",
    "\n",
    "Gen AI is a new field of AI that focuses on creating new content, such as images, music, and text. \n",
    "\n",
    "<img src=\"images/timeline-genai.png\" width=\"600\" height=\"400\" />\n",
    "\n",
    "## Gen AI Use Cases\n",
    "\n",
    "**Text Generation**: Create new text, such as articles, stories, or conversations.\n",
    "\n",
    "**Image Generation**: Create new images, such as photos, artwork, or designs.\n",
    "\n",
    "**Music Generation**: Compose music, melodies, or sound effects.\n",
    "\n",
    "**Video Generation**: Create new videos, such as animations or clips.\n",
    "\n",
    "**Data Generation**: Create synthetic data for training or testing.\n",
    "\n",
    "**Style Transfer**: Transfer styles between images, music, or text.\n",
    "\n",
    "**Image-to-Image Translation**: Translate images from one domain to another.\n",
    "\n",
    "**Text-to-Image Synthesis**: Generate images from text descriptions.\n",
    "\n",
    "**Dialogue Generation**: Engage in conversation, responding to user input.\n",
    "\n",
    "**Creative Writing**: Generate creative writing, such as poetry or short stories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning\n",
    "\n",
    "Deep learning is a subfield of machine learning that focuses on neural networks.\n",
    "\n",
    "Neural Networks are a type of model that is inspired by the human brain. They consist of layers of neurons that process input data and produce output data. Deep Neural Networks are neural networks with many layers. They are capable of learning complex patterns in data.\n",
    "\n",
    "<img src=\"images/nn.png\" width=\"600\" height=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks (RNNs)\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are a type of neural network that is designed to process sequences of data. They are commonly used for text and speech processing.\n",
    "\n",
    "<img src=\"images/RNN.png\" width=\"600\" height=\"400\" style=\" background-color: white;\" />\n",
    "\n",
    "**Backward Propagation**: RNNs use backpropagation to update their weights and biases during training. This involves computing the gradient of the loss function with respect to the weights and biases of the network.\n",
    "\n",
    "**Foward Propagation**: RNNs use forward propagation to compute the output of the network given an input sequence. This involves passing the input sequence through the network and computing the output at each time step.\n",
    "\n",
    "**Long Short-Term Memory (LSTM)**: LSTMs are a type of RNN that are designed to capture long-term dependencies in data. They are capable of learning patterns that are separated by long sequences of data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks (CNNs)\n",
    "\n",
    "Convolutional Neural Networks (CNNs) are a type of neural network that is designed to process images. They are commonly used for image recognition and computer vision.\n",
    "\n",
    "<img src=\"images/CNN.jpeg\" width=\"600\" height=\"400\" style=\" background-color: white;\" />\n",
    "\n",
    "CNNs use convolutional layers to extract features from images. These layers apply filters to the input image to detect patterns, such as edges, corners, and textures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers\n",
    "\n",
    "Transformers are a type of neural network architecture that has been successful in many Gen AI tasks. They are based on self-attention mechanisms that allow them to model long-range dependencies in data. Transformers have been used in many applications, such as language modeling, translation, and image generation.\n",
    "\n",
    "<img src=\"images/transformers.png\" width=\"600\" height=\"600\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "Embeddings are a way to represent data in a lower-dimensional space. They are commonly used in Gen AI to represent words, images, and other types of data. Embeddings are learned during training and capture the relationships between different data points.\n",
    "\n",
    "<img src=\"images/Transformers-attention-1.png\" />\n",
    "\n",
    "Using cosine similarity to find similar embeddings.\n",
    "\n",
    "## Embedding Code Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diego/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1290,  0.0631,  0.0218,  ..., -0.2822,  0.3723,  0.4550],\n",
      "         [ 0.7875, -0.0711, -0.3430,  ...,  0.0156,  0.5159,  0.1851],\n",
      "         [ 1.1745,  0.9952,  0.3751,  ...,  0.0757,  0.2636, -0.0634],\n",
      "         ...,\n",
      "         [-0.1707,  0.2353, -0.0354,  ..., -0.6519, -0.3293,  0.1394],\n",
      "         [ 0.1527, -0.3066,  0.0453,  ...,  0.2441,  0.2385, -0.5898],\n",
      "         [ 0.4515,  0.0422,  0.0441,  ...,  0.2707, -0.4147, -0.2581]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Define a sentence\n",
    "sentence = \"I love coding Gen AI applications.\"\n",
    "\n",
    "# Tokenize the sentence and obtain the input IDs\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "# Get the embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# The last hidden state is the first element of the output tuple\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "print(last_hidden_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Database\n",
    "\n",
    "A vector database is a database that stores vectors in a high-dimensional space. It allows you to query vectors based on their similarity to other vectors. This is useful for many Gen AI tasks, such as image search, recommendation systems, and content generation.\n",
    "\n",
    "<img src=\"images/vector_databases.png\" width=\"600\" height=\"400\" />\n",
    "\n",
    "Popular Vector Databases:\n",
    " * LanceDB\n",
    " * Pinecone\n",
    " * Chroma\n",
    " * OpenSearch\n",
    " * Redis\n",
    " * Postgres (pg_vector)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large Language Models\n",
    "\n",
    "Large language models are a type of Gen AI model that is trained on a large corpus of text data. They are capable of generating human-like text and engaging in conversation. Large language models have been used in many applications, such as chatbots, content generation, and translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatGPT\n",
    "\n",
    "GPT means: Generative Pre-trained Transformer.\n",
    "\n",
    "ChatGPT is a large language model (LLM) that is based on the GPT-series. It is capable of generating human-like text responses to user input. ChatGPT has been used in many applications, such as chatbots, conversational agents, and creative writing.\n",
    "\n",
    "<img src=\"images/GPT_VS_BERT.png\" />\n",
    "\n",
    "### GPT Cost to train\n",
    "\n",
    "| Model    | Parameters | Cost   |  Time    |\n",
    "| -------- | ---------- |--------|----------| \n",
    "| GPT-3    | 175B       | 4.6M   | 34 days  |\n",
    "| GPT-4    | 100T       | 2.6B   | 100 days |\n",
    "| LLAMA-2  | 70B        | 20M    | 23 days  |\n",
    "\n",
    "Llama-2 paper, it took 184,320 GPU hours of an A100 to train the model. 184320 hours = 7680 days ~= 21 years Renting AWS p4d. 24xlarge instance (8 GPUs) is $32.7726 per hour\n",
    "\n",
    "Cost References:\n",
    "* [GPT-4 Training days](https://towardsdatascience.com/the-carbon-footprint-of-gpt-4-d6c676eb21ae#:~:text=Recall%20that%20it's%20estimated%20that,to%202%2C600%20hours%20per%20server.)\n",
    "* [GPT-3 Cost](https://www.forbes.com/sites/craigsmith/2023/09/08/what-large-models-cost-you--there-is-no-free-ai-lunch/?sh=3561d9e24af7)\n",
    "* [GPT-3 Cost comments](https://ai.stackexchange.com/questions/43128/what-is-accelerated-years-in-describing-the-amount-of-the-training-time#:~:text=Since%20GPT%2D3%20is%20a,train%20the%20GPT%2D3%20model.)\n",
    "* [GPT Cost](https://www.moomoo.com/community/feed/109834449715205#:~:text=In%20terms%20of%20the%20training,reach%2046%20million%20U.S.%20dollars.)\n",
    "* [LLAMA 2 model card](https://github.com/meta-llama/llama/blob/main/MODEL_CARD.md)\n",
    "* [LLAMA 2 Cost](https://www.quora.com/How-much-money-did-Meta-cost-to-train-Llama-2-Why-has-NVIDIAs-stock-price-been-rising-and-how-high-could-it-ultimately-go#:~:text=Cost%20Breakdown%3A%20Meta%20allocated%20a,the%20training%20of%20Llama%202.)\n",
    "* [LLAMA2 Cost comments](https://www.linkedin.com/posts/hherry_according-to-the-llama-v2-paper-it-took-activity-7128798861606694912-FLaT/?trk=public_profile_like_view)\n",
    "* [LLAMA2 training days](https://news.ycombinator.com/item?id=35008694)\n",
    "\n",
    "Size of GPT-4 model\n",
    "\n",
    "<img src=\"images/GPT_4_metaphor.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Engineering\n",
    "\n",
    "Prompt engineering is a technique used to guide the output of a language model by providing a specific prompt. It involves designing prompts that encourage the model to generate desired responses. Prompt engineering is commonly used in chatbots, content generation, and translation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face\n",
    "\n",
    "Hugging Face is a company that specializes in natural language processing (NLP) and Gen AI. They provide a wide range of models, datasets, and tools for developers to use in their projects. Hugging Face is known for their Transformers library, which is a popular open-source library for working with transformer models. Huggingface is like Github for AI models.\n",
    "\n",
    "### Hugging Face Tasks\n",
    "\n",
    "<img src=\"images/hf-tasks.png\" width=\"600\" height=\"400\" />\n",
    "\n",
    "https://huggingface.co/tasks\n",
    "\n",
    "It's the real deal! Makes easy to consume models and perform common AI tasks on text, audio, image or video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diego/.local/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "2024-05-15 02:46:15.143305: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-15 02:46:16.220799: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-05-15 02:46:16.220951: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-05-15 02:46:16.220965: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "/home/diego/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: POSITIVE, with score: 0.9988325238227844\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Initialize the Hugging Face sentiment-analysis pipeline\n",
    "nlp = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# Use the pipeline to analyze this sentence\n",
    "result = nlp(\"I love using Generative AI\")[0]\n",
    "\n",
    "# Print the result\n",
    "print(f\"label: {result['label']}, with score: {result['score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ONNX - Open Neural Network Exchange\n",
    "\n",
    "ONNX is an open-source format for representing deep learning models. It allows models to be trained in one framework and deployed in another. ONNX is supported by many popular deep learning frameworks, such as PyTorch, TensorFlow, and MXNet.\n",
    "\n",
    "ONNX is a big deal because you can go from PyTorch <--> Tensorflow but also train a model in python and run inference on the model in Java.\n",
    "\n",
    "<img src=\"images/onnx.png\" width=\"600\" height=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain\n",
    "\n",
    "LangChain is a framework designed to simplify the creation of applications using large language models (LLMs). As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\n",
    "\n",
    "<img src=\"images/overall-langchain.jpeg\" width=\"600\" height=\"500\" />\n",
    "\n",
    "Java has [Langchain4J](https://docs.langchain4j.dev/) as langchain implementation for Java.\n",
    "\n",
    "Using Langchain we can do:\n",
    "* Smooth integration into your Java applications (or other languages), There is two-way integration between LLMs and Java: you can call LLMs from Java and allow LLMs to call your Java code in return.\n",
    "* Prompt Templating\n",
    "* Output parsing\n",
    "* Patterns like RaG or Agents\n",
    "* Vector Databases integration (Pinecone, OpenSearch, Redis, pg_vector, more...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RaG (Retrieval-augmented Generation)\n",
    "\n",
    "LLMs can halucinate.\n",
    "\n",
    "<img src=\"images/LLM_hallucinations.png\"  />\n",
    "\n",
    "RaG is approach that combines retrieval and generation models to improve the performance of language models. It uses a retrieval model to find relevant information from a large corpus of text and a generation model to generate responses based on that information.\n",
    "\n",
    "<img src=\"images/Rag-Simple.png\"  />\n",
    "\n",
    "RaG is key because it allows to generate more accurate and relevant responses by combining the strengths of retrieval and generation models. RaG also can reduce the cost of fine-tunning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KAN\n",
    "\n",
    "[KAN](https://arxiv.org/abs/2404.19756) it's a potential Alternative to MLP.\n",
    "\n",
    "<img src=\"images/KAN-simple.png\" />\n",
    "\n",
    "KANs diverge from traditional Multi-Layer Perceptrons (MLPs) by replacing fixed activation functions with learnable functions, effectively eliminating the need for linear weight matrices.\n",
    "\n",
    "<img src=\"images/KAN_VS_MLP.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stable Diffusion\n",
    "\n",
    "Stable Diffusion works by iteratively adding noise to an image to create a sequence of noisy images. The model then learns to denoise these images to recover the original image. This process is repeated multiple times to generate high-quality images.\n",
    "\n",
    "<img src=\"images/Stable_Diffusion_architecture.png\" />\n",
    "\n",
    "Stable Diffusion Common Models:\n",
    "* Stable Diffusion 3 (Stability AI)\n",
    "* DALL-E 3 (Open AI)\n",
    "* Imagine with Meta AI (Meta)\n",
    "* Midjourney\n",
    "\n",
    "Chalenges with Stable Diffusion:\n",
    "* Computational Intensity / High Hardware Demands\n",
    "* Quality Variance\n",
    "* Technical Complexity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Adversarial Networks (GAN)\n",
    "\n",
    "Generative Adversarial Networks (GANs) are a type of neural network that is designed to generate new data. They consist of two networks: a generator and a discriminator. The generator creates new data samples, while the discriminator tries to distinguish between real and fake samples.\n",
    "\n",
    "<img src=\"images/GANs.png\" style=\" background-color: white;\" />\n",
    "\n",
    "GAN Pros:\n",
    "  * Synthetic data generation\n",
    "  * High-quality results\n",
    "  * Versatility\n",
    "\n",
    "GAN Cons:\n",
    "  * Training Instability\n",
    "  * Computational Cost\n",
    "  * Overfitting\n",
    "  * Bias and Fairness\n",
    "  * Interpretability and Accountability\n",
    "\n",
    "Stable Diffusion vs GAN: The main difference between the two methods is their approach to generating new data. Stable Diffusion uses a process of adding and removing noise to an image, while GANs use a game-theoretic approach where two networks compete against each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Security\n",
    "\n",
    "[OWASP Top 10 for LLMs](https://owasp.org/www-project-top-10-for-large-language-model-applications/)\n",
    "\n",
    "LLM01: Prompt Injection\n",
    " * crafty inputs, causing unintended actions by the LLM\n",
    "\n",
    "LLM02: Insecure Output Handling\n",
    " * XSS, CSRF, SSRF\n",
    "\n",
    "LLM03: Training Data Poisoning\n",
    " * LLM training data is tampered\n",
    "\n",
    "LLM04: Model Denial of Service\n",
    " * Attackers cause resource-heavy operations on LLMs\n",
    "\n",
    "LLM05: Supply Chain Vulnerabilities\n",
    " * third-party datasets, pre- trained models, and plugins can add vulnerabilities\n",
    "\n",
    "LLM06: Sensitive Information Disclosure \n",
    " * LLMs may inadvertently reveal confidential data in their responses\n",
    " * Unauthorized data access, privacy violations, and security breaches.\n",
    "\n",
    "LLM07: Insecure Plugin Design\n",
    " * LLM plugins can have insecure inputs and insufficient access control\n",
    "\n",
    "LLM08: Excessive Agency\n",
    " * Excessive functionality, permissions, or autonomy granted to the LLM-based systems\n",
    "\n",
    "LLM09: Overreliance\n",
    " * Misinformation, miscommunication, legal issues, and security vulnerabilities\n",
    "\n",
    "LLM10: Model Theft\n",
    " * Unauthorized access, copying, or exfiltration of proprietary LLM models\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
